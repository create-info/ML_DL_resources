# -*- coding: utf-8 -*-
"""xgboost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iumySzLHi9BYXBguu3uJhap5erckC61m
"""

# Commented out IPython magic to ensure Python compatibility.
# 保险赔偿预测： https://www.kaggle.com/c/allstate-claims-severity/
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_absolute_error
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder,LabelBinarizer
from sklearn.model_selection import cross_validate

from scipy import stats
import seaborn as sns
from copy import deepcopy

# %matplotlib inline

# %config InlineBackend.figure_format = 'retina'

import xgboost as xgb

train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')

train.head()

train.shape

print('前20列:', list(train.columns[:20]))
print('后20列：', list(train.columns[-20:]))
# cat表示离散值，cont表示连续值
# 大概有116个离散属性，14个连续值属性，另外一个loss列和一个id列

train.describe()   # 查看数据中连续值特征的数目等信息

# 根据上面的数据。连续值特征的数值在[0-1]之间，且均值基本在0.5左右。
# 查看缺失值
pd.isnull(train).values.any()

# 查看数据集信息
train.info()

# 离散特征个数
cat_features = list(train.select_dtypes(
    include=['object']).columns)
print('离散特征个数为：',len(cat_features))

# 连续特征个数
cont_features = [cont for cont in list(train.select_dtypes(
    include=['float64','int64']).columns) if cont not in ['id','loss']]
print('离散特征个数为：',len(cont_features))

id_col = list(train.select_dtypes(include=['int64']).columns)
print('int64类型的特征数个数为：',len(id_col))

# 类别值中属性的个数
cat_uniques = []
for cat in cat_features:
  cat_uniques.append(len(train[cat].unique()))
uniq_values_in_cat = pd.DataFrame.from_items([('cat_name',cat_features),('unique_values',cat_uniques)])

uniq_values_in_cat.head()

# 画出各个特征属性个数
fig, (ax1, ax2) = plt.subplots(1,2)
fig.set_size_inches(16,5)
ax1.hist(uniq_values_in_cat.unique_values, bins=50)
ax1.set_title('Amount of categorical features with X distinct values')
ax1.set_xlabel('Distinct values in a feature')
ax1.set_ylabel('Features')
ax1.annotate('A feature with 326 vals', xy=(322, 2), xytext=(200, 38), arrowprops=dict(facecolor='black'))

ax2.set_xlim(2,30)
ax2.set_title('Zooming in the [0,30] part of left histogram')
ax2.set_xlabel('Distinct values in a feature')
ax2.set_ylabel('Features')
ax2.grid(True)
ax2.hist(uniq_values_in_cat[uniq_values_in_cat.unique_values <= 30].unique_values, bins=30)
ax2.annotate('Binary features', xy=(3, 71), xytext=(7, 71), arrowprops=dict(facecolor='black'))

# 大部分特征的属性值的个数是2个,（72/116）;88个/116特征的属性值少于4个，另外有一个特征有326个值。
# loss列（赔偿值），即类别标签
plt.figure(figsize=(16,8))
plt.plot(train['id'], train['loss'])
plt.title('Loss values per id')
plt.xlabel('id')
plt.ylabel('loss')
plt.legend()
plt.show()

# 损失值中有一个显著的峰值表示严重事故这样的数据分布将导致回归效果不好
# 计算loss列的偏度(度量了实数值随机变量的均值分布的不对称性)
stats.mstats.skew(train['loss']).data

# 数据确实是倾斜的
# 对数据进行对数变换通常可以改善倾斜，可以使用 np.log
stats.mstats.skew(np.log(train['loss'])).data

fig, (ax1, ax2) = plt.subplots(1,2)
fig.set_size_inches(16,5)
ax1.hist(train['loss'], bins=50)
ax1.set_title('Train Loss target histogram')
ax1.grid(True)
# 对每个loss值取对数
ax2.hist(np.log(train['loss']), bins=50, color='g')
ax2.set_title('Train Log Loss target histogram')
ax2.grid(True)
plt.show()

# 经过转换之后确实符合正态分布了，有利于对结果的预测

# 特征之间的相关性
# 这里只看连续值特征：
plt.subplots(figsize=(16,9))
correlation_mat = train[cont_features].corr()
# 热力图,相关系数越接近1，表示两个特征越相关
sns.heatmap(correlation_mat, annot=True)

# 我们看到几个特征之间有很高的相关性
threshold = 0.7
for i in range(len(correlation_mat)):
    for j in range(len(correlation_mat)):
        if (i>j) and (np.abs(correlation_mat.iloc[i,j])>threshold):
            print ("%s and %s = %.2f" % (test.columns[i],test.columns[j],correlation_mat.iloc[i,j]))

# 有需要的话，可以把相关性特别高的特征去掉，避免贡献率过高导致预测效果不好
# 到这里数据预处理已经完成了，接下来建立XGBoost模型进行预测：

train['log_loss'] = np.log(train['loss'])

features = [x for x in train.columns if x not in ['id','loss', 'log_loss']]
train_x = train[cont_features]
train_y = train['log_loss']

# 首先，我们训练一个基本的xgboost模型，然后进行参数调节通过交叉验证来观察结果的变换，使用平均绝对误差来衡量
# mean_absolute_error(np.exp(y), np.exp(yhat))。
# xgboost 自定义了一个数据矩阵类 DMatrix，会在训练开始时进行一遍预处理，从而提高之后每次迭代的效率
def xg_eval_mae(yhat, dtrain):
    y = dtrain.get_label()
    return 'mae', mean_absolute_error(np.exp(y), np.exp(yhat))

# 数据变成适应Xgboost的格式
dtrain = xgb.DMatrix(train_x, train['log_loss'])
xgb_params = {
    'seed': 0,  # 随机种子
    'eta': 0.1, # 如同学习率
    'colsample_bytree': 0.5, # 生成树时进行的列采样
    'silent': 1, # 设置成1则没有运行信息输出，最好是设置为0.
    'subsample': 0.5,  # 随机采样训练样本
    'objective': 'reg:linear', # 线性回归的问题
    'max_depth': 5, # 构建树的深度，越大越容易过拟合
    'min_child_weight': 3 # 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束
}

# 交叉验证
     
bst_cv1 = xgb.cv(xgb_params, dtrain, num_boost_round=50, nfold=3, seed=0, 
                feval=xg_eval_mae, maximize=False, early_stopping_rounds=10)

# print(bst_cv1)
# 取最后一行输出
print('CV score:', bst_cv1.iloc[-1,:]['test-mae-mean'])

# 画出训练集和测试集平均绝对误差
plt.figure()
bst_cv1[['train-mae-mean', 'test-mae-mean']].plot()

# Commented out IPython magic to ensure Python compatibility.
# # 第一个基础模型：
# # 只建立了50个树模型
# # 没有发生过拟合
# %%time
# # 建立100个树模型
# bst_cv2 = xgb.cv(xgb_params, dtrain, num_boost_round=100, 
#                 nfold=3, seed=0, feval=xg_eval_mae, maximize=False, 
#                 early_stopping_rounds=10)
# 
# print ('CV score:', bst_cv2.iloc[-1,:]['test-mae-mean'])

# 画出xgboost的训练过程中的误差变化过程
fig, (ax1, ax2) = plt.subplots(1,2)
fig.set_size_inches(16,4)

ax1.set_title('100 rounds of training')
ax1.set_xlabel('Rounds')
ax1.set_ylabel('Loss')
ax1.grid(True)
ax1.plot(bst_cv2[['train-mae-mean', 'test-mae-mean']])
ax1.legend(['Training Loss', 'Test Loss'])

ax2.set_title('60 last rounds of training')
ax2.set_xlabel('Rounds')
ax2.set_ylabel('Loss')
ax2.grid(True)
ax2.plot(bst_cv2.iloc[40:][['train-mae-mean', 'test-mae-mean']])
ax2.legend(['Training Loss', 'Test Loss'])

# 现在开始调节其他参数
# 1.选择一组初始参数
# 2.改变 max_depth 和 min_child_weight.
# 3.调节 gamma 降低模型过拟合风险.
# 4.调节 subsample 和 colsample_bytree 改变数据采样策略.
# 5.调节学习率 eta.
# 建立一个调参的帮助类：
class XGBoostRegressor(object):
    def __init__(self, **kwargs):
        self.params = kwargs
        if 'num_boost_round' in self.params:
            self.num_boost_round = self.params['num_boost_round']
        self.params.update({'silent': 1, 'objective': 'reg:linear', 'seed': 0})
        
    def fit(self, x_train, y_train):
        dtrain = xgb.DMatrix(x_train, y_train)
        self.bst = xgb.train(params=self.params, dtrain=dtrain, num_boost_round=self.num_boost_round,
                             feval=xg_eval_mae, maximize=False)
        
    def predict(self, x_pred):
        dpred = xgb.DMatrix(x_pred)
        return self.bst.predict(dpred)
    
    def kfold(self, x_train, y_train, nfold=5):
        dtrain = xgb.DMatrix(x_train, y_train)
        cv_rounds = xgb.cv(params=self.params, dtrain=dtrain, num_boost_round=self.num_boost_round,
                           nfold=nfold, feval=xg_eval_mae, maximize=False, early_stopping_rounds=10)
        return cv_rounds.iloc[-1,:]
    
    def plot_feature_importances(self):
        feat_imp = pd.Series(self.bst.get_fscore()).sort_values(ascending=False)
        feat_imp.plot(title='Feature Importances')
        plt.ylabel('Feature Importance Score')
        
    def get_params(self, deep=True):
        return self.params
 
    def set_params(self, **params):
        self.params.update(params)
        return self

    def mae_score(y_true, y_pred):
      return mean_absolute_error(np.exp(y_true), np.exp(y_pred))

from sklearn.metrics import mean_absolute_error, make_scorer
mae_scorer = make_scorer(mae_score, greater_is_better=False)

bst = XGBoostRegressor(eta=0.1, colsample_bytree=0.5, subsample=0.5, 
                       max_depth=5, min_child_weight=3, num_boost_round=50)

bst.kfold(train_x, train_y, nfold=5)

# 2.树的深度与节点权重
# 这些参数对xgboost性能影响最大，因此，它们应该调整。
# max_depth: 树的最大深度。增加这个值会使模型更加复杂，也容易出现过拟合，深度3-10是合理的。
# min_child_weight: 正则化参数. 如果树分区中的实例权重小于定义的总和，则停止树构建过程。
xgb_param_grid = {'max_depth': list(range(4,9)), 'min_child_weight': list((1,3,6))}
xgb_param_grid['max_depth']

# Commented out IPython magic to ensure Python compatibility.
# # 网格搜索交叉验证
# %%time
# from sklearn.model_selection import GridSearchCV
# 
# grid = GridSearchCV(XGBoostRegressor(eta=0.1, num_boost_round=50, colsample_bytree=0.5, subsample=0.5),
#                 param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)
# grid.fit(train_x, train_y.values)

print(grid.best_params_)
print(grid.best_score_)

# Commented out IPython magic to ensure Python compatibility.
# # 3.调节gamma去降低过拟合风险
# %%time
# xgb_param_grid = {'gamma':[ 0.1 * i for i in range(0,5)]}
# grid = GridSearchCV(XGBoostRegressor(eta=0.1, num_boost_round=50, max_depth=8, min_child_weight=3,
# colsample_bytree=0.5, subsample=0.5),param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)
# grid.fit(train_x, train_y.values)

print(grid.best_params_)
print(grid.best_score_)

# Commented out IPython magic to ensure Python compatibility.
# # 4.调节样本采样方式 subsample 和 colsample_bytree
# %%time
# xgb_param_grid = {'subsample':[ 0.1 * i for i in range(6,9)],
# 'colsample_bytree':[ 0.1 * i for i in range(6,9)]}
# grid = GridSearchCV(XGBoostRegressor(eta=0.1, gamma=0.3, num_boost_round=50, max_depth=8, min_child_weight=3),
#                     param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)
# grid.fit(train_x, train_y.values)

print(grid.best_params_)
print(grid.best_score_)

# Commented out IPython magic to ensure Python compatibility.
# # 5.减小学习率并增大树个数
# # 参数优化的最后一步是降低学习速度，同时增加更多的估计量
# %%time
#     
# xgb_param_grid = {'eta':[0.5,0.4,0.3,0.2,0.1,0.075,0.05,0.04,0.03]}
# grid = GridSearchCV(XGBoostRegressor(num_boost_round=50, gamma=0.3, max_depth=8, min_child_weight=3,
# colsample_bytree=0.7, subsample=0.8),param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)
# 
# grid.fit(train_x, train_y.values)

print(grid.cv_results_)
print(grid.best_params_)
print(grid.best_score_)

# 画出结果
eta, y = np.array([ item['eta'] for item in grid.cv_results_['params']]), grid.cv_results_['mean_test_score']
plt.figure(figsize=(10,4))
plt.title('MAE and ETA, 50 trees')
plt.xlabel('eta')
plt.ylabel('score')
plt.plot(eta, -y)
plt.grid(True)
plt.show()

# np.array([for eta in grid.cv_results_['params']['eta']])
np.array([ item['eta'] for item in grid.cv_results_['params']])

# 把树的个数增加到100
xgb_param_grid = {'eta':[0.5,0.4,0.3,0.2,0.1,0.075,0.05,0.04,0.03]}
grid = GridSearchCV(XGBoostRegressor(num_boost_round=100, gamma=0.3, max_depth=8, min_child_weight=3,
colsample_bytree=0.7, subsample=0.8),param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)
grid.fit(train_x, train_y.values)

print(grid.best_params_)
print(grid.best_score_)

# 画出结果
eta, y = np.array([ item['eta'] for item in grid.cv_results_['params']]), grid.cv_results_['mean_test_score']
plt.figure(figsize=(10,4))
plt.title('MAE and ETA, 100 trees')
plt.xlabel('eta')
plt.ylabel('score')
plt.plot(eta, -y)
plt.grid(True)
plt.show()

# 调参结束了，基本可以按照调整好的参数对测试集的参数进行预测